{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "imdb_dir = r'C:/Users/clare\\Documents/aclImdb'\r\n",
    "train_dir = os.path.join(imdb_dir, 'train')\r\n",
    "\r\n",
    "labels = []\r\n",
    "texts = []\r\n",
    "\r\n",
    "for label_type in ['neg', 'pos']:\r\n",
    "    dir_name = os.path.join(train_dir, label_type)\r\n",
    "    for fname in os.listdir(dir_name):\r\n",
    "        if fname[-4:] == \".txt\":\r\n",
    "            f = open(os.path.join(dir_name, fname), encoding='utf8')\r\n",
    "            texts.append(f.read())\r\n",
    "            f.close()\r\n",
    "            if label_type == 'neg':\r\n",
    "                labels.append(0)\r\n",
    "            else:\r\n",
    "                labels.append(1)\r\n",
    "\r\n",
    "print(len(texts))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "maxlen = 100\r\n",
    "training_samples = 200\r\n",
    "validation_samples = 10000\r\n",
    "max_words = 10000\r\n",
    "\r\n",
    "tokenizer = Tokenizer(num_words=max_words)\r\n",
    "tokenizer.fit_on_texts(texts)\r\n",
    "sequences = tokenizer.texts_to_sequences(texts)\r\n",
    "\r\n",
    "word_index = tokenizer.word_index\r\n",
    "\r\n",
    "print('共用了 %s 個 token 字詞.' % len(word_index))\r\n",
    "\r\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\r\n",
    "labels = np.asarray(labels)\r\n",
    "\r\n",
    "print(\"資料張量：\", data.shape)\r\n",
    "print(\"標籤張量：\", labels.shape)\r\n",
    "\r\n",
    "indices = np.arange(data.shape[0])\r\n",
    "np.random.shuffle(indices)\r\n",
    "data = data[indices]\r\n",
    "labels = labels[indices]\r\n",
    "\r\n",
    "x_train = data[:training_samples]\r\n",
    "y_train = labels[:training_samples]\r\n",
    "x_val = data[training_samples: training_samples + validation_samples]\r\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "共用了 88582 個 token 字詞.\n",
      "資料張量： (25000, 100)\n",
      "標籤張量： (25000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "glove_dir = r'C:/Users/clare/Documents'\r\n",
    "\r\n",
    "embeddings_index = {}\r\n",
    "f = open(os.path.join(glove_dir, r'glove.6B.100d.txt'), encoding='UTF-8')\r\n",
    "for line in f:\r\n",
    "    values = line.split()\r\n",
    "    word = values[0]\r\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
    "    embeddings_index[word] = coefs\r\n",
    "f.close()\r\n",
    "\r\n",
    "print(\"共有 %s 個嵌入向量.\" % len(embeddings_index))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "共有 400000 個嵌入向量.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "embedding_dim = 100\r\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\r\n",
    "print(embedding_matrix.shape)\r\n",
    "for word, i in word_index.items():\r\n",
    "    if i < max_words:\r\n",
    "        embedding_vector = embeddings_index.get(word)\r\n",
    "        if embedding_vector is not None:\r\n",
    "            embedding_matrix[i] = embedding_vector"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Embedding, Flatten, Dense\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(32, activation='relu'))\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\r\n",
    "model.layers[0].trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.compile(optimizer='rmsprop',\r\n",
    "              loss='binary_crossentropy',\r\n",
    "              metrics=['acc'])\r\n",
    "history = model.fit(x_train, y_train, epochs=10,\r\n",
    "                    batch_size=32, validation_data=(x_val, y_val))\r\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 1.8264 - acc: 0.4600 - val_loss: 0.7892 - val_acc: 0.5008\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.6672 - acc: 0.6400 - val_loss: 0.6880 - val_acc: 0.5551\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 0.3675 - acc: 0.8750 - val_loss: 1.2133 - val_acc: 0.5006\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.3246 - acc: 0.9150 - val_loss: 0.6841 - val_acc: 0.5739\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.2762 - acc: 0.9300 - val_loss: 0.9045 - val_acc: 0.5151\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.1502 - acc: 0.9900 - val_loss: 0.7363 - val_acc: 0.5585\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 71ms/step - loss: 0.1044 - acc: 0.9950 - val_loss: 1.9804 - val_acc: 0.5004\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.2018 - acc: 0.9200 - val_loss: 1.0322 - val_acc: 0.5285\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0430 - acc: 1.0000 - val_loss: 0.8088 - val_acc: 0.5746\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.8152 - val_acc: 0.5708\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "acc = history.history['acc']\r\n",
    "val_acc = history.history['val_acc']\r\n",
    "loss = history.history['loss']\r\n",
    "val_loss = history.history['val_loss']\r\n",
    "\r\n",
    "epochs = range(1, len(acc) + 1)\r\n",
    "\r\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\r\n",
    "plt.plot(epochs, val_acc, 'b', label='Validaion acc')\r\n",
    "plt.title('Training and validation accuracy')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "\r\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\r\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\r\n",
    "plt.title('Training and validation loss')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "plt.figure()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}