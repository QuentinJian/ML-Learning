{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\r\n",
    "\r\n",
    "token_index = {}\r\n",
    "\r\n",
    "for sample in samples:\r\n",
    "    for word in sample.split():\r\n",
    "        print(sample)\r\n",
    "        print(word)\r\n",
    "        if word not in token_index:\r\n",
    "            token_index[word] = len(token_index) + 1\r\n",
    "\r\n",
    "\r\n",
    "max_length = 10\r\n",
    "\r\n",
    "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\r\n",
    "\r\n",
    "print(results.shape)\r\n",
    "\r\n",
    "for i, sample in enumerate(samples):\r\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\r\n",
    "        index = token_index.get(word)\r\n",
    "        results[i, j, index] = 1."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The cat sat on the mat.\n",
      "The\n",
      "The cat sat on the mat.\n",
      "cat\n",
      "The cat sat on the mat.\n",
      "sat\n",
      "The cat sat on the mat.\n",
      "on\n",
      "The cat sat on the mat.\n",
      "the\n",
      "The cat sat on the mat.\n",
      "mat.\n",
      "The dog ate my homework.\n",
      "The\n",
      "The dog ate my homework.\n",
      "dog\n",
      "The dog ate my homework.\n",
      "ate\n",
      "The dog ate my homework.\n",
      "my\n",
      "The dog ate my homework.\n",
      "homework.\n",
      "(2, 10, 11)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import string\r\n",
    "\r\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\r\n",
    "characters = string.printable\r\n",
    "print(len(characters))\r\n",
    "\r\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\r\n",
    "\r\n",
    "max_length = 50\r\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\r\n",
    "\r\n",
    "print(results.shape)\r\n",
    "for i, sample in enumerate(samples):\r\n",
    "    for j, character in enumerate(sample):\r\n",
    "        index = token_index.get(character)\r\n",
    "        results[i, j, index] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from keras.preprocessing.text import Tokenizer\r\n",
    "\r\n",
    "samples = ['The cat sat on the mat', 'The dog ate my homework.']\r\n",
    "\r\n",
    "tokenizer = Tokenizer(num_words=1000)\r\n",
    "tokenizer.fit_on_texts(samples)\r\n",
    "sequences = tokenizer.texts_to_sequences(samples)\r\n",
    "\r\n",
    "print(sequences)\r\n",
    "\r\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\r\n",
    "\r\n",
    "print(one_hot_results.shape)\r\n",
    "\r\n",
    "print(one_hot_results[0][:15])\r\n",
    "print(one_hot_results[1][:15])\r\n",
    "\r\n",
    "word_index = tokenizer.word_index\r\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "(2, 1000)\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples = ['The cat sat in the mat.', 'The dog ate my homework.']\r\n",
    "\r\n",
    "dimensionality = 1000\r\n",
    "max_length = 10\r\n",
    "\r\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\r\n",
    "for i, sample in enumerate(samples):\r\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\r\n",
    "        index = abs(hash(word)) % dimensionality\r\n",
    "        results[i, j, index] = 1\r\n",
    "print(results.shape)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}